{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translation-tfrecord-0.ipynb",
      "provenance": [],
      "mount_file_id": "1kiqe0a3c3GFNjyIqclMscl1KwCmHqG-x",
      "authorship_tag": "ABX9TyPI/ibAhD/INIUU6UqOEDWA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kp425/nlp_lab/blob/master/TFrecord%20for%20translation%20task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qmXBpUI0Ls6"
      },
      "source": [
        "!git clone https://github.com/kp425/utilities.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxuewskgkgM8",
        "outputId": "c777a1e5-1f25-4af2-a4a3-05242981e75c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import json\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download('punkt')\n",
        "from utilities import timer\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_NqSooznuF8"
      },
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
        "                               as_supervised=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJZouk5UuX9f"
      },
      "source": [
        "train_data, val_data, test_data = examples['train'], examples['validation'], examples['test']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejt6KCW3A3oS"
      },
      "source": [
        "# WordLevel tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQl3lipBEstd"
      },
      "source": [
        "class WordTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 0\n",
        "        self.vocab2id, self.id2vocab = {}, {}\n",
        "\n",
        "    def add_tokens(self, tokens):\n",
        "        count = self.vocab_size\n",
        "        for tkn in tokens:\n",
        "            if tkn not in self.vocab2id:\n",
        "                self.vocab2id.update({tkn:count})\n",
        "                count+=1\n",
        "        self.id2vocab = dict((v,k) for k,v in self.vocab2id.items())\n",
        "        self.vocab_size = count\n",
        "  \n",
        "    def encode(self, list_of_words):\n",
        "        return [self.vocab2id[i] for i in list_of_words]\n",
        "    \n",
        "    def decode(self, list_of_ints):\n",
        "        return ''.join([self.id2vocab[i] for i in list_of_ints])\n",
        "    \n",
        "    def __getitem__(self, key):\n",
        "        if type(key)==str:\n",
        "            return self.vocab2id[key]\n",
        "        elif type(key)==int:\n",
        "            return self.id2vocab[key]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6SvmMDOIIAf",
        "outputId": "fad2e418-1963-4806-efde-10dc48402d60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "TRIAN_SAMPLE_SIZE = 10000\n",
        "TRAIN_MAX_SEQ_LEN = None\n",
        "VAL_SAMPLE_SIZE = None\n",
        "VAL_MAX_SEQ_LEN = None\n",
        "TEST_SAMPLE_SIZE =  None\n",
        "TEST_MAX_SEQ_LEN =  None\n",
        "\n",
        "lang1_tkn = WordTokenizer()\n",
        "lang2_tkn = WordTokenizer()\n",
        "\n",
        "sos = '<start>'\n",
        "eos = '<end>'\n",
        "lang1_tkn.add_tokens([sos,eos])\n",
        "lang2_tkn.add_tokens([sos,eos])\n",
        "\n",
        "\n",
        "@timer\n",
        "def collect_vocab(ds, sample_size = None, langs = ['portuguese','english'], max_length_seq_allowed = None):\n",
        "    lang1_seqs, lang2_seqs = [], []\n",
        "    if sample_size != None:\n",
        "        ds = ds.take(sample_size)\n",
        "    for pt,en in ds:\n",
        "        pt = pt.numpy().decode('utf-8')\n",
        "        en = en.numpy().decode('utf-8')\n",
        "        pt = word_tokenize(pt, language= langs[0])\n",
        "        en = word_tokenize(en, language= langs[1])\n",
        "        if max_length_seq_allowed != None:\n",
        "            if len(en)<=max_length_seq_allowed:\n",
        "                lang1_seqs.append(en)\n",
        "                lang2_seqs.append(pt)\n",
        "                lang1_tkn.add_tokens(en)\n",
        "                lang2_tkn.add_tokens(pt)\n",
        "        else:\n",
        "            lang1_seqs.append(en)\n",
        "            lang2_seqs.append(pt)\n",
        "            lang1_tkn.add_tokens(en)\n",
        "            lang2_tkn.add_tokens(pt)\n",
        "\n",
        "    return lang1_seqs, lang2_seqs\n",
        "\n",
        "\n",
        "train1, train2 = collect_vocab(train_data, \n",
        "                               sample_size = TRIAN_SAMPLE_SIZE, \n",
        "                               max_length_seq_allowed = TRAIN_MAX_SEQ_LEN)\n",
        "\n",
        "val1, val2 = collect_vocab(val_data,\n",
        "                            sample_size = VAL_SAMPLE_SIZE, \n",
        "                            max_length_seq_allowed = VAL_MAX_SEQ_LEN)\n",
        "\n",
        "test1, test2 = collect_vocab(test_data,\n",
        "                             sample_size = TEST_SAMPLE_SIZE, \n",
        "                             max_length_seq_allowed = TEST_MAX_SEQ_LEN)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total execution time: 25635 ms\n",
            "Total execution time: 5217 ms\n",
            "Total execution time: 8208 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUDnSvnKLlCB",
        "outputId": "bbd20849-1c19-4748-95a5-82d8eb74ba61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "print(train1[0])\n",
        "print(val1[0])\n",
        "print(test1[0])\n",
        "\n",
        "print(train2[0])\n",
        "print(val2[0])\n",
        "print(test2[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'when', 'you', 'improve', 'searchability', ',', 'you', 'actually', 'take', 'away', 'the', 'one', 'advantage', 'of', 'print', ',', 'which', 'is', 'serendipity', '.']\n",
            "['did', 'they', 'eat', 'fish', 'and', 'chips', '?']\n",
            "['then', ',', 'predictions', 'can', 'be', 'made', 'and', 'tested', '.']\n",
            "['e', 'quando', 'melhoramos', 'a', 'procura', ',', 'tiramos', 'a', 'única', 'vantagem', 'da', 'impressão', ',', 'que', 'é', 'a', 'serendipidade', '.']\n",
            "['tinham', 'comido', 'peixe', 'com', 'batatas', 'fritas', '?']\n",
            "['depois', ',', 'podem', 'fazer-se', 'e', 'testar-se', 'previsões', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpJuFpqgItJ_"
      },
      "source": [
        "def encode_n_pad(lang1_seqs, lang2_seqs):\n",
        "    enc_lang1, enc_lang2 = [], []\n",
        "    for i,j in zip(lang1_seqs, lang2_seqs):\n",
        "        #adding teacher forcing tokens here\n",
        "        i = [sos]+i+[eos]\n",
        "        j = [sos]+j+[sos]\n",
        "        enc_lang1.append(lang1_tkn.encode(i))\n",
        "        enc_lang2.append(lang2_tkn.encode(j))\n",
        "\n",
        "    padded_lang1 = tf.keras.preprocessing.sequence.pad_sequences(enc_lang1, padding=\"post\")\n",
        "    padded_lang2 = tf.keras.preprocessing.sequence.pad_sequences(enc_lang2, padding=\"post\")\n",
        "    return padded_lang1, padded_lang2\n",
        "\n",
        "    \n",
        "train_enc1, train_enc2 = encode_n_pad(train1, train2)\n",
        "val_enc1, val_enc2 = encode_n_pad(val1, val2)\n",
        "test_enc1, test_enc2 = encode_n_pad(test1, test2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5OkuPkkkl2u",
        "outputId": "b35ab265-bd17-43f5-ab3f-02e567211cb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(len(train_enc1[0]))\n",
        "print(len(val_enc1[0]))\n",
        "print(len(test_enc1[0]))\n",
        "\n",
        "print(len(train_enc2[0]))\n",
        "print(len(val_enc2[0]))\n",
        "print(len(test_enc2[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "201\n",
            "116\n",
            "137\n",
            "193\n",
            "122\n",
            "117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeNXPYNIoiZi"
      },
      "source": [
        "def _bytes_feature(value):\n",
        "  if isinstance(value, type(tf.constant(0))):\n",
        "    value = value.numpy()\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _int64_feature(values):\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value = [val]))\n",
        "\n",
        "def serialize_to_tfr(lang1, lang2, lang1_enc, lang2_enc):\n",
        "    def _serialize_seqs(lang1, lang2, lang1_enc, lang2_enc):\n",
        "        feature = {'lang1': _bytes_feature(lang1),\n",
        "             'lang2': _bytes_feature(lang2),\n",
        "             'lang1_enc': _bytes_feature(lang1_enc),\n",
        "             'lang2_enc': _bytes_feature(lang2_enc)}\n",
        "\n",
        "        example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "        return example_proto.SerializeToString()\n",
        "    \n",
        "    tf_string = tf.py_function(_serialize_seqs ,\n",
        "                               (lang1, lang2, lang1_enc, lang2_enc), \n",
        "                               tf.string)      \n",
        "    return tf.reshape(tf_string, ()) \n",
        "\n",
        "\n",
        "def parse_from_tfr(element):\n",
        "\n",
        "    feature_description = {'lang1': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "        'lang2': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "        'lang1_enc': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "        'lang2_enc': tf.io.FixedLenFeature([], tf.string, default_value=''),}\n",
        "    \n",
        "    #output = tf.io.parse_single_example(element, feature_description)\n",
        "    output = tf.io.parse_example(element, feature_description)\n",
        "    lang1 = tf.io.parse_tensor(output['lang1'], out_type = tf.string)\n",
        "    lang2 = tf.io.parse_tensor(output['lang2'], out_type = tf.string)\n",
        "    lang1_enc = tf.io.parse_tensor(output['lang1_enc'], out_type = tf.int32)\n",
        "    lang2_enc = tf.io.parse_tensor(output['lang2_enc'], out_type = tf.int32)\n",
        "    \n",
        "    return lang1, lang2, lang1_enc, lang2_enc\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYMIaSxiKDHq"
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((\n",
        "                list(map(tf.io.serialize_tensor, train1)),\n",
        "                list(map(tf.io.serialize_tensor, train2)),\n",
        "                list(map(tf.io.serialize_tensor, train_enc1)),\n",
        "                list(map(tf.io.serialize_tensor, train_enc2))))\n",
        "train_ds = train_ds.map(serialize_to_tfr)\n",
        "\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((\n",
        "                list(map(tf.io.serialize_tensor, val1)),\n",
        "                list(map(tf.io.serialize_tensor, val2)),\n",
        "                list(map(tf.io.serialize_tensor, val_enc1)),\n",
        "                list(map(tf.io.serialize_tensor, val_enc2))))\n",
        "val_ds = val_ds.map(serialize_to_tfr)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((\n",
        "                list(map(tf.io.serialize_tensor, test1)),\n",
        "                list(map(tf.io.serialize_tensor, test2)),\n",
        "                list(map(tf.io.serialize_tensor, test_enc1)),\n",
        "                list(map(tf.io.serialize_tensor, test_enc2))))\n",
        "test_ds = test_ds.map(serialize_to_tfr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sample_limit = \"all\" if TRIAN_SAMPLE_SIZE==None else TRIAN_SAMPLE_SIZE\n",
        "seq_limit = \"all\" if TRAIN_MAX_SEQ_LEN==None else TRAIN_MAX_SEQ_LEN\n",
        "\n",
        "folder = f'/content/drive/My Drive/Colab Notebooks/Data dump/en-pt/{sample_limit}_{seq_limit}'\n",
        "\n",
        "os.makedirs(folder)\n",
        "\n",
        "\n",
        "train_name = \"train.tfrecord\"\n",
        "val_name =   \"val.tfrecord\"\n",
        "test_name =  \"test.tfrecord\"\n",
        "\n",
        "\n",
        "writer = tf.data.experimental.TFRecordWriter(os.path.join(folder, train_name))\n",
        "writer.write(train_ds)\n",
        "\n",
        "writer = tf.data.experimental.TFRecordWriter(os.path.join(folder, val_name))\n",
        "writer.write(val_ds)\n",
        "\n",
        "writer = tf.data.experimental.TFRecordWriter(os.path.join(folder, test_name))\n",
        "writer.write(test_ds)\n",
        "\n",
        "\n",
        "tkns = {'lang1':[lang1_tkn.vocab2id, lang1_tkn.id2vocab, lang1_tkn.vocab_size],\n",
        " 'lang2':[lang2_tkn.vocab2id, lang2_tkn.id2vocab, lang2_tkn.vocab_size]}\n",
        " \n",
        "\n",
        "with open(os.path.join(folder,'tkns.json'), 'w') as f:\n",
        "    json.dump(tkns, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1OsKGBsBC86"
      },
      "source": [
        "# CharLevelTokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6IvDjNtBRxV"
      },
      "source": [
        "class CharTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 0\n",
        "        self.vocab2id, self.id2vocab = {}, {}\n",
        "\n",
        "    def add_tokens(self, tokens):\n",
        "        count = self.vocab_size\n",
        "        for tkn in tokens:\n",
        "            if tkn not in self.vocab2id:\n",
        "                self.vocab2id.update({tkn:count})\n",
        "                count+=1\n",
        "        self.id2vocab = dict((v,k) for k,v in self.vocab2id.items())\n",
        "        self.vocab_size = count\n",
        "  \n",
        "    def encode(self, list_of_words):\n",
        "        return [self.vocab2id[i] for i in list_of_words]\n",
        "    \n",
        "    def decode(self, list_of_ints):\n",
        "        return ''.join([self.id2vocab[i] for i in list_of_ints])\n",
        "    \n",
        "    def __getitem__(self, key):\n",
        "        if type(key)==str:\n",
        "            return self.vocab2id[key]\n",
        "        elif type(key)==int:\n",
        "            return self.id2vocab[key]\n",
        "\n",
        "TRIAN_SAMPLE_SIZE = None\n",
        "TRAIN_MAX_SEQ_LEN = None\n",
        "VAL_SAMPLE_SIZE = None\n",
        "VAL_MAX_SEQ_LEN = None\n",
        "TEST_SAMPLE_SIZE =  None\n",
        "TEST_MAX_SEQ_LEN =  None\n",
        "\n",
        "lang1_tkn = CharTokenizer()\n",
        "lang2_tkn = CharTokenizer()\n",
        "\n",
        "sos = '<start>'\n",
        "eos = '<end>'\n",
        "lang1_tkn.add_tokens([sos,eos])\n",
        "lang2_tkn.add_tokens([sos,eos])\n",
        "\n",
        "\n",
        "def collect_vocab(ds, sample_size = None, max_length_seq_allowed = None):\n",
        "    lang1_seqs, lang2_seqs = [], []\n",
        "    if sample_size != None:\n",
        "        ds = ds.take(sample_size)\n",
        "    for pt,en in ds:\n",
        "        pt = pt.numpy().decode('utf-8')\n",
        "        en = en.numpy().decode('utf-8')\n",
        "        \n",
        "        if max_length_seq_allowed != None:\n",
        "            if len(en)<=max_length_seq_allowed:\n",
        "                lang1_seqs.append(en)\n",
        "                lang2_seqs.append(pt)\n",
        "                lang1_tkn.add_tokens(set(en))\n",
        "                lang2_tkn.add_tokens(set(pt))\n",
        "        else:\n",
        "            lang1_seqs.append(en)\n",
        "            lang2_seqs.append(pt)\n",
        "            lang1_tkn.add_tokens(set(en))\n",
        "            lang2_tkn.add_tokens(set(pt))\n",
        "\n",
        "    return lang1_seqs, lang2_seqs\n",
        "\n",
        "\n",
        "def encode_n_pad(lang1_seqs, lang2_seqs):\n",
        "    enc_lang1, enc_lang2 = [], []\n",
        "    for i,j in zip(lang1_seqs, lang2_seqs):\n",
        "        #adding teacher forcing tokens here\n",
        "        i = [sos]+list(i)+[eos]\n",
        "        j = [sos]+list(j)+[eos]\n",
        "        enc_lang1.append(lang1_tkn.encode(i))\n",
        "        enc_lang2.append(lang2_tkn.encode(j))\n",
        "\n",
        "    padded_lang1 = tf.keras.preprocessing.sequence.pad_sequences(enc_lang1, padding=\"post\")\n",
        "    padded_lang2 = tf.keras.preprocessing.sequence.pad_sequences(enc_lang2, padding=\"post\")\n",
        "    return padded_lang1, padded_lang2\n",
        "\n",
        "\n",
        "train1, train2 = collect_vocab(train_data, sample_size = None, max_length_seq_allowed = None)\n",
        "val1, val2 = collect_vocab(val_data)\n",
        "test1, test2 = collect_vocab(test_data)  \n",
        "\n",
        "train_enc1, train_enc2 = encode_n_pad(train1, train2)\n",
        "val_enc1, val_enc2 = encode_n_pad(val1, val2)\n",
        "test_enc1, test_enc2 = encode_n_pad(test1, test2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD7kraBjDDR4",
        "outputId": "f05db419-e54a-4ded-858e-d5bc22ef19a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'w'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svfQ95cCCEZ9"
      },
      "source": [
        "def _bytes_feature(value):\n",
        "  if isinstance(value, type(tf.constant(0))):\n",
        "    value = value.numpy()\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _int64_feature(values):\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value = [val]))\n",
        "\n",
        "def serialize_to_tfr(lang1, lang2, lang1_enc, lang2_enc):\n",
        "    def _serialize_seqs(lang1, lang2, lang1_enc, lang2_enc):\n",
        "        feature = {'lang1': _bytes_feature(lang1),\n",
        "             'lang2': _bytes_feature(lang2),\n",
        "             'lang1_enc': _bytes_feature(lang1_enc),\n",
        "             'lang2_enc': _bytes_feature(lang2_enc)}\n",
        "\n",
        "        example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "        return example_proto.SerializeToString()\n",
        "    \n",
        "    tf_string = tf.py_function(_serialize_seqs ,\n",
        "                               (lang1, lang2, lang1_enc, lang2_enc), \n",
        "                               tf.string)      \n",
        "    return tf.reshape(tf_string, ()) \n",
        "\n",
        "\n",
        "def parse_from_tfr(element):\n",
        "\n",
        "    feature_description = {'lang1': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "        'lang2': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "        'lang1_enc': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
        "        'lang2_enc': tf.io.FixedLenFeature([], tf.string, default_value=''),}\n",
        "    \n",
        "    #output = tf.io.parse_single_example(element, feature_description)\n",
        "    output = tf.io.parse_example(element, feature_description)\n",
        "    lang1 = tf.io.parse_tensor(output['lang1'], out_type = tf.string)\n",
        "    lang2 = tf.io.parse_tensor(output['lang2'], out_type = tf.string)\n",
        "    lang1_enc = tf.io.parse_tensor(output['lang1_enc'], out_type = tf.int32)\n",
        "    lang2_enc = tf.io.parse_tensor(output['lang2_enc'], out_type = tf.int32)\n",
        "    \n",
        "    return lang1, lang2, lang1_enc, lang2_enc\n",
        "\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((\n",
        "                list(map(tf.io.serialize_tensor, train1)),\n",
        "                list(map(tf.io.serialize_tensor, train2)),\n",
        "                list(map(tf.io.serialize_tensor, train_enc1)),\n",
        "                list(map(tf.io.serialize_tensor, train_enc2))))\n",
        "train_ds = train_ds.map(serialize_to_tfr)\n",
        "\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((\n",
        "                list(map(tf.io.serialize_tensor, val1)),\n",
        "                list(map(tf.io.serialize_tensor, val2)),\n",
        "                list(map(tf.io.serialize_tensor, val_enc1)),\n",
        "                list(map(tf.io.serialize_tensor, val_enc2))))\n",
        "val_ds = val_ds.map(serialize_to_tfr)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((\n",
        "                list(map(tf.io.serialize_tensor, test1)),\n",
        "                list(map(tf.io.serialize_tensor, test2)),\n",
        "                list(map(tf.io.serialize_tensor, test_enc1)),\n",
        "                list(map(tf.io.serialize_tensor, test_enc2))))\n",
        "test_ds = test_ds.map(serialize_to_tfr)\n",
        "\n",
        "\n",
        "sample_limit = \"all\" if TRIAN_SAMPLE_SIZE==None else TRIAN_SAMPLE_SIZE\n",
        "seq_limit = \"all\" if TRAIN_MAX_SEQ_LEN==None else TRAIN_MAX_SEQ_LEN\n",
        "\n",
        "\n",
        "folder = f'/content/drive/My Drive/Colab Notebooks/Data dump/en-pt/{sample_limit}_{seq_limit}'\n",
        "\n",
        "os.makedirs(folder)\n",
        "\n",
        "\n",
        "train_name = \"train.tfrecord\"\n",
        "val_name =   \"val.tfrecord\"\n",
        "test_name =  \"test.tfrecord\"\n",
        "\n",
        "\n",
        "writer = tf.data.experimental.TFRecordWriter(os.path.join(folder, train_name))\n",
        "writer.write(train_ds)\n",
        "\n",
        "writer = tf.data.experimental.TFRecordWriter(os.path.join(folder, val_name))\n",
        "writer.write(val_ds)\n",
        "\n",
        "writer = tf.data.experimental.TFRecordWriter(os.path.join(folder, test_name))\n",
        "writer.write(test_ds)\n",
        "\n",
        "\n",
        "tkns = {'lang1':[lang1_tkn.vocab2id, lang1_tkn.id2vocab, lang1_tkn.vocab_size],\n",
        " 'lang2':[lang2_tkn.vocab2id, lang2_tkn.id2vocab, lang2_tkn.vocab_size]}\n",
        " \n",
        "\n",
        "with open(os.path.join(folder,'tkns.json'), 'w') as f:\n",
        "    json.dump(tkns, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LfSLqN8L8_E"
      },
      "source": [
        "#Split a tfrecord into multiple tfrecords\n",
        "\n",
        "raw_dataset = tf.data.TFRecordDataset(os.path.join(folder, train_name))\n",
        "\n",
        "shards = 5\n",
        "\n",
        "for i in range(shards):\n",
        "    writer = tf.data.experimental.TFRecordWriter(folder+\"/\"+f\"train{i}.tfrecord\")\n",
        "    writer.write(raw_dataset.shard(shards, i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jylI6ToSM2oO",
        "outputId": "d2a3fbad-f0be-454e-a532-ba89da8099cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "shrds = 7\n",
        "dd = tf.data.Dataset.range(100)\n",
        "\n",
        "for i in range(shrds):\n",
        "    tmp = dd.shard(shrds, i)\n",
        "    print([i.numpy() for i in tmp])\n",
        "\n",
        "print(train_ds.cardinality())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 91, 98]\n",
            "[1, 8, 15, 22, 29, 36, 43, 50, 57, 64, 71, 78, 85, 92, 99]\n",
            "[2, 9, 16, 23, 30, 37, 44, 51, 58, 65, 72, 79, 86, 93]\n",
            "[3, 10, 17, 24, 31, 38, 45, 52, 59, 66, 73, 80, 87, 94]\n",
            "[4, 11, 18, 25, 32, 39, 46, 53, 60, 67, 74, 81, 88, 95]\n",
            "[5, 12, 19, 26, 33, 40, 47, 54, 61, 68, 75, 82, 89, 96]\n",
            "[6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97]\n",
            "tf.Tensor(51785, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYquEG2ZCFUo"
      },
      "source": [
        "# Benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj2zNDLq0I3j",
        "outputId": "3368aa04-4e61-40ff-dba4-442855532e6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "b = [np.arange(0,500) for _ in range(30000)]\n",
        "\n",
        "a = list(map(tf.io.serialize_tensor, b))\n",
        "\n",
        "#Map is faster than apply\n",
        "\n",
        "@timer\n",
        "def f1():\n",
        "    d = tf.data.Dataset.from_tensor_slices((a,a,a,a)).apply(lambda x: x.map(serialize_to_tfr))\n",
        "    print(\"here\")\n",
        "    filename = '/content/d1.tfrecord'\n",
        "    writer = tf.data.experimental.TFRecordWriter(filename)\n",
        "    writer.write(d)   \n",
        "\n",
        "@timer\n",
        "def f2():\n",
        "    d = tf.data.Dataset.from_tensor_slices((a,a,a,a)).map(serialize_to_tfr)\n",
        "    filename = '/content/d2.tfrecord'\n",
        "    writer = tf.data.experimental.TFRecordWriter(filename)\n",
        "    writer.write(d)     \n",
        "\n",
        "f1()\n",
        "f2()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "here\n",
            "Total execution time: 18846 ms\n",
            "Total execution time: 18396 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dSb19sazBh5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-fcgxUh8wTx"
      },
      "source": [
        "os.makedirs(\"/sex/pavani\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}